# -*- coding: utf-8 -*-
"""Sampling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14ZhgezxbdUOQ0C8u-OUeuE8kMx-DWpa_
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

# Step 1: Load the dataset
data = pd.read_csv('Creditcard_data.csv')
X = data.drop('Class', axis=1)
y = data['Class']

# Step 2: Balance the dataset
oversampler = RandomOverSampler(random_state=42)
undersampler = RandomUnderSampler(random_state=42)
X_over, y_over = oversampler.fit_resample(X, y)
X_under, y_under = undersampler.fit_resample(X, y)

# Step 3: Sample size detection formula
def calculate_sample_size(total, margin_of_error=0.05, confidence=0.95):
    z = 1.96  # 95% confidence level
    p = 0.5  # Maximum variability
    n = (z**2 * p * (1 - p)) / (margin_of_error**2)
    return min(int(n), total)

sample_size = calculate_sample_size(len(X))

# Step 4: Sampling techniques

# Simple Random Sampling
X_simple, _, y_simple, _ = train_test_split(X_over, y_over, train_size=sample_size, random_state=1)

# Stratified Sampling
X_stratified, _, y_stratified, _ = train_test_split(X, y, train_size=sample_size, stratify=y, random_state=3)

# Cluster Sampling
def cluster_sample(X, y, clusters, size):
    groups = np.array_split(X.index, clusters)
    selected = np.concatenate(groups[:size // (len(X) // clusters)])
    return X.loc[selected], y.loc[selected]

X_cluster, y_cluster = cluster_sample(X_over, y_over, 10, sample_size)

# Bootstrap Sampling
def bootstrap_sample(X, y, size):
    indices = np.random.choice(len(X), size=size, replace=True)
    return X.iloc[indices], y.iloc[indices]

X_bootstrap, y_bootstrap = bootstrap_sample(X_over, y_over, sample_size)

# Systematic Sampling
def systematic_sample(X, y, size):
    step = len(X) // size
    indices = np.arange(0, len(X), step)[:size]
    return X.iloc[indices], y.iloc[indices]

X_systematic, y_systematic = systematic_sample(X_over, y_over, sample_size)

# Step 5: Train models
models = {
    "M1": RandomForestClassifier(random_state=42),
    "M2": LogisticRegression(max_iter=1000),
    "M3": SVC(),
    "M4": KNeighborsClassifier(),
    "M5": DecisionTreeClassifier(random_state=42)
}

samples = {
    "Sampling1": (X_simple, y_simple),
    "Sampling2": (X_stratified, y_stratified),
    "Sampling3": (X_cluster, y_cluster),
    "Sampling4": (X_bootstrap, y_bootstrap),
    "Sampling5": (X_systematic, y_systematic)
}

results = []
for model_name, model in models.items():
    for sample_name, (X_samp, y_samp) in samples.items():
        X_train, X_test, y_train, y_test = train_test_split(X_samp, y_samp, test_size=0.3, random_state=42)
        model.fit(X_train, y_train)
        accuracy = accuracy_score(y_test, model.predict(X_test))
        results.append({"Model": model_name, "Sample": sample_name, "Accuracy": round(accuracy * 100, 2)})

# Step 6: Results
results_df = pd.DataFrame(results)
best_results = results_df.loc[results_df.groupby("Model")["Accuracy"].idxmax()]

# Formatting the output for table-like summary
print("All Results:")
print(results_df.pivot(index="Model", columns="Sample", values="Accuracy"))

print("\nBest Results:")
print(best_results)

# Save results to CSV
results_df.to_csv('all_resultss.csv', index=False)
best_results.to_csv('best_results.csv', index=False)

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from imblearn.over_sampling import RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler

# Step 1: Load the dataset
data = pd.read_csv('Creditcard_data.csv')
X = data.drop('Class', axis=1)
y = data['Class']

# Step 2: Balance the dataset
oversampler = RandomOverSampler(random_state=42)
undersampler = RandomUnderSampler(random_state=42)
X_over, y_over = oversampler.fit_resample(X, y)
X_under, y_under = undersampler.fit_resample(X, y)

# Step 3: Sample size detection formula
def calculate_sample_size(total, margin_of_error=0.05, confidence=0.95):
    z = 1.96  # 95% confidence level
    p = 0.5  # Maximum variability
    n = (z**2 * p * (1 - p)) / (margin_of_error**2)
    return min(int(n), total)

sample_size = calculate_sample_size(len(X))

# Step 4: Sampling techniques

# Simple Random Sampling
X_simple, _, y_simple, _ = train_test_split(X_over, y_over, train_size=sample_size, random_state=1)

# Stratified Sampling
X_stratified, _, y_stratified, _ = train_test_split(X, y, train_size=sample_size, stratify=y, random_state=3)

# Cluster Sampling
def cluster_sample(X, y, clusters, size):
    groups = np.array_split(X.index, clusters)
    selected = np.concatenate(groups[:size // (len(X) // clusters)])
    return X.loc[selected], y.loc[selected]

X_cluster, y_cluster = cluster_sample(X_over, y_over, 10, sample_size)

# Bootstrap Sampling
def bootstrap_sample(X, y, size):
    indices = np.random.choice(len(X), size=size, replace=True)
    return X.iloc[indices], y.iloc[indices]

X_bootstrap, y_bootstrap = bootstrap_sample(X_over, y_over, sample_size)

# Systematic Sampling
def systematic_sample(X, y, size):
    step = len(X) // size
    indices = np.arange(0, len(X), step)[:size]
    return X.iloc[indices], y.iloc[indices]

X_systematic, y_systematic = systematic_sample(X_over, y_over, sample_size)

# Step 5: Train models
models = {
    "M1": RandomForestClassifier(random_state=42),
    "M2": LogisticRegression(max_iter=1000),
    "M3": SVC(),
    "M4": KNeighborsClassifier(),
    "M5": DecisionTreeClassifier(random_state=42)
}

samples = {
    "Sampling1": (X_simple, y_simple),
    "Sampling2": (X_stratified, y_stratified),
    "Sampling3": (X_cluster, y_cluster),
    "Sampling4": (X_bootstrap, y_bootstrap),
    "Sampling5": (X_systematic, y_systematic)
}

results = []
for model_name, model in models.items():
    for sample_name, (X_samp, y_samp) in samples.items():
        X_train, X_test, y_train, y_test = train_test_split(X_samp, y_samp, test_size=0.3, random_state=42)
        model.fit(X_train, y_train)
        accuracy = accuracy_score(y_test, model.predict(X_test))
        results.append({"Model": model_name, "Sample": sample_name, "Accuracy": round(accuracy * 100, 2)})

# Step 6: Results
results_df = pd.DataFrame(results)
best_results = results_df.loc[results_df.groupby("Model")["Accuracy"].idxmax()]

# Pivoting the DataFrame for "all_results.csv"
all_results_pivot = results_df.pivot(index="Model", columns="Sample", values="Accuracy")
all_results_pivot.reset_index(inplace=True)

# Formatting the output for table-like summary
print("All Results:")
print(all_results_pivot)

print("\nBest Results:")
print(best_results)

# Save results to CSV
all_results_pivot.to_csv('all_results.csv', index=False)
best_results.to_csv('best_results.csv', index=False)